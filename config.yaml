# LLM Inference Server Configuration

# API Settings
api:
  host: "0.0.0.0"
  port: 8000
  debug: false
  cors_origins:
    - "*" # Update with specific origins for production

# Model Settings
models:
  - name: "llama-3.2-3b"
    type: "mlx"
    path: "mlx-community/Llama-3.2-3B-Instruct-4bit"
    preload: true # Load at startup
    default: true # Use as default model

  - name: "llama-3.2-8b"
    type: "mlx"
    path: "mlx-community/Llama-3.2-8B-Instruct-4bit"
    preload: false

  - name: "mistral-7b"
    type: "mlx"
    path: "mlx-community/Mistral-7B-v0.1-4bit"
    preload: false

# Task Scheduler Settings
task_scheduler:
  max_concurrent_tasks: 4 # Limit concurrent tasks
  task_max_age_hours: 24 # How long to keep completed tasks
  cleanup_interval_hours: 6 # How often to clean up old tasks

# Generation Defaults
generation:
  max_tokens: 512
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.1

# Cache Settings
cache:
  enable: true
  max_size: 100 # Maximum number of cached responses
  ttl_seconds: 3600 # Time to live for cached items
