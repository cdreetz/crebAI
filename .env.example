# API Settings
HOST=0.0.0.0
PORT=8000
DEBUG=false
VERSION=1.0.0
PROJECT_NAME="LLM Inference Server"

# CORS Settings
CORS_ORIGINS=*

# LLM Settings
DEFAULT_MODEL_NAME=llama-3.2-3b
MODEL_CACHE_DIR=~/.cache/mlx-models

# Task Settings
TASK_CLEANUP_INTERVAL_HOURS=6
TASK_MAX_AGE_HOURS=24
